{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62ea1b6",
   "metadata": {},
   "source": [
    "First Program in OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc145c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all relevant libraries...\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499a47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    #env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06efdfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  Discrete(2)\n",
      "Observation Space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "#Printing Observation space and Action Space...\n",
    "print('Action Space: ',env.action_space)\n",
    "print('Observation Space: ',env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d2e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sohaib bin Kashif\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the Loop...\n"
     ]
    }
   ],
   "source": [
    "#Implementing an Action on the environment by the Agent...\n",
    "env.reset()\n",
    "for step_time in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "print(\"End of the Loop...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85894b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step : 0\n",
      "Observation : [ 0.03358055 -0.15518603 -0.02996103  0.29888698]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 1\n",
      "Observation : [ 0.03047683  0.0403499  -0.02398329 -0.00309251]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 2\n",
      "Observation : [ 0.03128383  0.23580745 -0.02404514 -0.30324492]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 3\n",
      "Observation : [ 0.03599998  0.04103629 -0.03011004 -0.01824124]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 4\n",
      "Observation : [ 0.03682071  0.23657683 -0.03047487 -0.3202701 ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 5\n",
      "Observation : [ 0.04155224  0.04190183 -0.03688027 -0.03735143]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 6\n",
      "Observation : [ 0.04239028  0.2375327  -0.03762729 -0.34143853]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 7\n",
      "Observation : [ 0.04714093  0.04296574 -0.04445607 -0.06085449]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 8\n",
      "Observation : [ 0.04800025  0.23869598 -0.04567316 -0.36722553]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 9\n",
      "Observation : [ 0.05277417  0.43443617 -0.05301767 -0.6739529 ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 10\n",
      "Observation : [ 0.06146289  0.2400896  -0.06649672 -0.39842254]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 11\n",
      "Observation : [ 0.06626468  0.4360888  -0.07446518 -0.71130717]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 12\n",
      "Observation : [ 0.07498646  0.24207269 -0.08869132 -0.44296208]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 13\n",
      "Observation : [ 0.07982791  0.43833032 -0.09755056 -0.76223254]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 14\n",
      "Observation : [ 0.08859452  0.24467774 -0.11279521 -0.50176907]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 15\n",
      "Observation : [ 0.09348807  0.44119382 -0.12283059 -0.82776153]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 16\n",
      "Observation : [ 0.10231195  0.6377619  -0.13938582 -1.1564136 ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 17\n",
      "Observation : [ 0.11506718  0.44470453 -0.16251409 -0.910483  ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 18\n",
      "Observation : [ 0.12396128  0.2521105  -0.18072376 -0.6729676 ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 19\n",
      "Observation : [ 0.1290035   0.0598993  -0.19418311 -0.44219038]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 20\n",
      "Observation : [ 0.13020147 -0.13202117 -0.20302692 -0.21645193]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 21\n",
      "Observation : [ 0.12756105 -0.32374963 -0.20735595  0.00595312]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 22\n",
      "Observation : [ 0.12108606 -0.515388   -0.20723689  0.22672622]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 23\n",
      "Observation : [ 0.1107783  -0.7070389  -0.20270237  0.44755897]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 24\n",
      "Observation : [ 0.09663752 -0.50971335 -0.19375119  0.09844351]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 25\n",
      "Observation : [ 0.08644325 -0.31241876 -0.19178231 -0.24856812]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 26\n",
      "Observation : [ 0.08019488 -0.11514921 -0.19675368 -0.59508634]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 27\n",
      "Observation : [ 0.07789189  0.08210311 -0.2086554  -0.9427293 ]\n",
      "Reward : 1.0\n",
      "Done : False\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "Step : 28\n",
      "Observation : [ 0.07953396  0.27933255 -0.22750999 -1.2930572 ]\n",
      "Reward : 1.0\n",
      "Done : True\n",
      "Info : {}\n",
      "\n",
      "\n",
      "\n",
      "End of the Loop...\n"
     ]
    }
   ],
   "source": [
    "#Implementing an Action on the environment by the Agent...\n",
    "env.reset()\n",
    "for step_time in range(1000):\n",
    "    env.render()\n",
    "    \n",
    "    #env.step(env.action_space.sample()) # take a random action\n",
    "    #Rewriting this line of code...\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(\"Step : {}\".format(step_time))\n",
    "    print(\"Observation : {}\".format(observation))\n",
    "    print(\"Reward : {}\".format(reward))\n",
    "    print(\"Done : {}\".format(done))\n",
    "    print(\"Info : {}\".format(info))\n",
    "    print(\"\\n\\n\")\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"End of the Loop...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e884324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now applying our Deep Learning Model into the mix...\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "#Initialising some variables for upcoming functions\n",
    "goal_steps = 500          #Pick any large number of steps; we want the game to reach its completion.\n",
    "score_requirement = 60    #Scores whould be greater than the threshold mentioned.\n",
    "no_of_games = 10000     #No. of instances as the number of games played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df421275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Creating T1 Data...\n",
    "\n",
    "def model_data_prep():\n",
    "    T1_data = []\n",
    "    scores_list = []\n",
    "    for game in range(no_of_games):\n",
    "        score = 0\n",
    "        game_memory = []         \n",
    "        prev_observation = []\n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0,2)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(prev_observation) > 0:\n",
    "                #Stores previous observation(Postion of Cartpole) and action...\n",
    "                game_memory.append([prev_observation, action])    \n",
    "                \n",
    "            prev_observation = observation\n",
    "            score += reward\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                    \n",
    "        if score >= score_requirement:\n",
    "            scores_list.append(score)\n",
    "                \n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                T1_data.append([data[0],output])\n",
    "                    \n",
    "        env.reset()\n",
    "        \n",
    "    print(scores_list)\n",
    "    \n",
    "    return T1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a53483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74.0, 74.0, 62.0, 65.0, 142.0, 75.0, 66.0, 74.0, 72.0, 62.0, 72.0, 69.0, 66.0, 61.0, 65.0, 85.0, 70.0, 76.0, 62.0, 61.0, 60.0, 60.0, 88.0, 64.0, 103.0, 64.0, 70.0, 64.0, 72.0, 75.0, 60.0, 80.0, 70.0, 74.0, 69.0, 67.0, 61.0, 65.0, 67.0, 75.0, 60.0, 81.0, 82.0, 67.0, 93.0, 87.0, 67.0, 83.0, 65.0, 61.0, 63.0, 60.0, 89.0, 64.0, 61.0, 78.0, 70.0, 71.0, 74.0, 62.0, 90.0, 60.0, 61.0, 65.0, 76.0, 95.0, 115.0, 72.0, 78.0, 65.0, 71.0, 67.0, 71.0, 71.0, 64.0, 76.0, 76.0, 66.0, 63.0, 71.0, 61.0, 65.0, 70.0, 71.0, 62.0, 63.0, 63.0, 87.0, 66.0, 61.0, 65.0, 70.0, 66.0, 74.0, 68.0, 66.0, 67.0, 90.0, 61.0, 73.0, 62.0, 72.0, 83.0, 62.0, 62.0, 79.0, 87.0, 63.0, 73.0, 62.0, 71.0, 60.0, 78.0, 62.0, 75.0, 61.0, 70.0, 60.0, 75.0, 60.0, 60.0, 60.0, 73.0, 109.0, 67.0, 61.0, 66.0, 71.0, 65.0, 87.0, 87.0, 85.0, 61.0, 60.0, 70.0, 60.0, 76.0, 66.0, 79.0, 66.0, 65.0, 69.0, 62.0, 60.0, 85.0, 71.0, 115.0, 67.0, 64.0, 76.0, 84.0, 66.0, 66.0, 65.0, 69.0, 75.0, 107.0, 83.0, 68.0, 60.0, 80.0, 63.0, 69.0, 62.0, 89.0, 64.0, 79.0, 63.0, 79.0, 69.0]\n"
     ]
    }
   ],
   "source": [
    "T1_Data = model_data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ba5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for initialising a Deep Learning Neural Network...\n",
    "\n",
    "def DL_Model(model, input_size, output_size):\n",
    "    \n",
    "    model.add(Dense(128, input_dim = input_size, activation = 'relu'))\n",
    "    model.add(Dense(52, activation = 'relu'))\n",
    "    model.add(Dense(output_size, activation = 'linear'))\n",
    "    model.compile( loss = 'mse', optimizer=Adam())\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84f82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the previous function in our training function...\n",
    "\n",
    "def Train_DL_Model(T1_data):\n",
    "    X = np.array([i[0] for i in T1_data]).reshape(-1, len(T1_data[0][0]))\n",
    "    Y = np.array([i[1] for i in T1_data]).reshape(-1, len(T1_data[0][1]))\n",
    "    \n",
    "    Model = Sequential()\n",
    "    DL_Model(Model, len(X[0]), len(Y[0]))\n",
    "    \n",
    "    Model.fit(X,Y, epochs=10)\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb4bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2509\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2346\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2333\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2325\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2327\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2325\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2320\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2323\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2319\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x188ca4fed90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = Train_DL_Model(T1_Data)   #???\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5630167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "Average Scores:  200.0\n",
      "choice 1: 0.5  choice 0: 0.5\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "\n",
    "for each_game in range(5):\n",
    "    score = 0\n",
    "    prev_obs = []\n",
    "    for step_index in range(goal_steps):\n",
    "        env.render()\n",
    "        \n",
    "        if(len(prev_obs) == 0):\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1,len(prev_obs))))\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores)\n",
    "print(\"Average Scores: \",sum(scores)/len(scores))\n",
    "print(\"choice 1: {}  choice 0: {}\".format(choices.count(1)/len(choices),choices.count(0)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ef796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
